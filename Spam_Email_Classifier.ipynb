{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_Email_Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1T-kPdEvF_D"
      },
      "source": [
        "# Spam Email Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5pZrdrIvl8a"
      },
      "source": [
        "## Defining the Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXpzFK91vqwq"
      },
      "source": [
        "### Specifying the Data Analytic Question\r\n",
        "\r\n",
        "> Using Naive Bayes Algorithm to classify whether an email is spam or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cYbiar0v9Gc"
      },
      "source": [
        "### Defining the Metric for Success\r\n",
        "\r\n",
        "> Prediction accuracy of over 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSRDgH6ivazs"
      },
      "source": [
        "### Understanding the context\r\n",
        "\r\n",
        "> The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...\r\n",
        "\r\n",
        "> The collection of spam e-mails came from the creators' postmaster and individuals who had filed spam. The collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JswxrSHwafq"
      },
      "source": [
        "### Experimental Design\r\n",
        "\r\n",
        "* Load Data\r\n",
        "* Data Cleaning\r\n",
        "* Exploratory Data Analysis\r\n",
        "* Data Modelling\r\n",
        "* Model Evaluation\r\n",
        "* Model improvement and tuning\r\n",
        "* Challenging the solution\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOg9Scfmw0MS"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vudcgxD8UgV7"
      },
      "source": [
        "# Importing our libraries \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "sHC6Hb86Wrzs",
        "outputId": "6569a2cb-d777-4f4b-a3d9-6afbace8ca69"
      },
      "source": [
        "# Loading and previewing our dataset\r\n",
        "cols = ['word_freq_make','word_freq_address','word_freq_all','word_freq_3d',          \r\n",
        "          'word_freq_our','word_freq_over','word_freq_remove','word_freq_internet',     \r\n",
        "          'word_freq_order','word_freq_mail','word_freq_receive','word_freq_will',         \r\n",
        "          'word_freq_people','word_freq_report','word_freq_addresses','word_freq_free',         \r\n",
        "          'word_freq_business','word_freq_email','word_freq_you','word_freq_credit',       \r\n",
        "          'word_freq_your','word_freq_font','word_freq_000','word_freq_money','word_freq_hp',           \r\n",
        "          'word_freq_hpl','word_freq_george','word_freq_650','word_freq_lab','word_freq_labs',         \r\n",
        "          'word_freq_telnet','word_freq_857','word_freq_data','word_freq_415','word_freq_85',           \r\n",
        "          'word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm','word_freq_direct',\r\n",
        "          'word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project','word_freq_re',           \r\n",
        "          'word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(',            \r\n",
        "          'char_freq_[','char_freq_!','char_freq_$','char_freq_#','capital_run_length_average', \r\n",
        "          'capital_run_length_longest','capital_run_length_total','spam']\r\n",
        "\r\n",
        "data = pd.read_csv('spambase.data', names=cols)\r\n",
        "data.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.00               0.64  ...                       278     1\n",
              "1            0.21               0.28  ...                      1028     1\n",
              "2            0.06               0.00  ...                      2259     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "4            0.00               0.00  ...                       191     1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyIJDvkxdgkj"
      },
      "source": [
        "# Dropping columns counting punctuation marks\r\n",
        "data = data.drop(['char_freq_;','char_freq_(','char_freq_[','char_freq_!','char_freq_$','char_freq_#'], axis=1)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdXRxxezd3MK",
        "outputId": "3f771f3f-add4-451c-9094-f9e4c8252412"
      },
      "source": [
        "# Shape\r\n",
        "data.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4601, 52)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtyWNwZVeuxk",
        "outputId": "44db4364-e920-44bb-bceb-58ca96ed4573"
      },
      "source": [
        "# Check for duplicates\r\n",
        "data.duplicated().sum()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qYJSULge4Sz"
      },
      "source": [
        "# Dropping the duplicates\r\n",
        "data.drop_duplicates(inplace=True)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "yaoy78rFgYxE",
        "outputId": "5c58d75e-3c62-45e4-cb17-d920edb7ee84"
      },
      "source": [
        "plt.figure('Counts')\r\n",
        "plt.title('Counts')\r\n",
        "data['spam'].value_counts(normalize=True).plot(kind='bar')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa586626b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CLl7XYihMld"
      },
      "source": [
        "> More than 60% of the data is not spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfw_B056hv8s"
      },
      "source": [
        "# Determining the X and Y\r\n",
        "X = data.drop(['spam'], axis = 1)\r\n",
        "y = data['spam']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPyrVzrfiNO8",
        "outputId": "e3130d58-6fbf-42e7-d76f-b4eaea696d9c"
      },
      "source": [
        "# Splitting the data\r\n",
        "\r\n",
        "# Apply smote to x and y\r\n",
        "smote = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=7)\r\n",
        "X, y = smote.fit_resample(X, y)\r\n",
        "\r\n",
        "# Split data into train and test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=23)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1VsJNrpnVE"
      },
      "source": [
        "# Scaling data\r\n",
        "scaler = MinMaxScaler().fit(X_train)\r\n",
        "X_train = scaler.transform(X_train)\r\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHxGRB7FlpHb"
      },
      "source": [
        "# Fitting our model\r\n",
        "multi = MultinomialNB().fit(X_train, y_train)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6u06-7qmGEi",
        "outputId": "65bdde83-bc1a-4da0-92b5-d9061b0f8a60"
      },
      "source": [
        "# Evaluating the Model\r\n",
        "predicted = multi.predict(X_test)\r\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.863681592039801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V416bRClq8V7",
        "outputId": "dde2428b-53d5-4479-d9b2-36a220c8c6a7"
      },
      "source": [
        "# Confusion Matrix\r\n",
        "confusion_matrix(y_test, predicted)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[386, 111],\n",
              "       [ 26, 482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kawx4r-UqguT",
        "outputId": "534bcfc3-dd1f-4104-e4b9-635d82aec265"
      },
      "source": [
        "# Model Evaluation\r\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.78      0.85       497\n",
            "           1       0.81      0.95      0.88       508\n",
            "\n",
            "    accuracy                           0.86      1005\n",
            "   macro avg       0.87      0.86      0.86      1005\n",
            "weighted avg       0.87      0.86      0.86      1005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv7VcvC2rlFU"
      },
      "source": [
        "**Repeating the steps with 30% and 40% Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMjEj9xaruIz",
        "outputId": "34d83cf7-2060-4db3-f557-c0ecb2a5e9fd"
      },
      "source": [
        "test_sizes = [0.3, 0.4]\r\n",
        "\r\n",
        "for size in test_sizes:\r\n",
        "  smote = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=7)\r\n",
        "  X, y = smote.fit_resample(X, y)\r\n",
        "  print('Test Size: ', size*100, '%')\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, random_state=23)\r\n",
        "\r\n",
        "  scaler = MinMaxScaler().fit(X_train)\r\n",
        "  X_train = scaler.transform(X_train)\r\n",
        "  X_test = scaler.transform(X_test)\r\n",
        "\r\n",
        "  predicted = multi.predict(X_test)\r\n",
        "  print('Accuracy: ', np.mean(predicted == y_test))\r\n",
        "\r\n",
        "  print('CONFUSION MATRIX')\r\n",
        "  print(confusion_matrix(y_test, predicted))\r\n",
        "\r\n",
        "  print('Further Evaluation\\n')\r\n",
        "  print(classification_report(y_test, predicted))\r\n",
        "  print('\\n\\n')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Size:  30.0 %\n",
            "Accuracy:  0.8653846153846154\n",
            "CONFUSION MATRIX\n",
            "[[583 170]\n",
            " [ 33 722]]\n",
            "Further Evaluation\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.77      0.85       753\n",
            "           1       0.81      0.96      0.88       755\n",
            "\n",
            "    accuracy                           0.87      1508\n",
            "   macro avg       0.88      0.87      0.86      1508\n",
            "weighted avg       0.88      0.87      0.86      1508\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Test Size:  40.0 %\n",
            "Accuracy:  0.8691542288557214\n",
            "CONFUSION MATRIX\n",
            "[[792 223]\n",
            " [ 40 955]]\n",
            "Further Evaluation\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.78      0.86      1015\n",
            "           1       0.81      0.96      0.88       995\n",
            "\n",
            "    accuracy                           0.87      2010\n",
            "   macro avg       0.88      0.87      0.87      2010\n",
            "weighted avg       0.88      0.87      0.87      2010\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMOCxHCXtO40"
      },
      "source": [
        "* 30% and 40% test data size gives the same accuracy of 86.92% which is higher than that of test data size 20%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kgYw9pst8lb"
      },
      "source": [
        "## Challenging the Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d_cRPQZuAND"
      },
      "source": [
        "> Principal Componet Analysis may be applied to identify the features that contibutes the most in determining whether an email is spam or not."
      ]
    }
  ]
}